# CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks

[Website](https://mete.is/crow) | [Leaderboard](https://mete.is/crow/leaderboard) | [Download data](https://mete.is/crow/tasks)

CRoW is a multi-task benchmark to evaluate commonsense reasoning ability of NLP systems in solving real-world tasks where this ability is required.

This repo contains the code used to build [CRoW benchmark](https://mete.is/crow) and evaluate models on it. If you would like to download the data for this benchmark and evaluate your own models on it, please check out the [Tasks](https://mete.is/crow/tasks) section. We also keep an active [leaderboard](https://mete.is/crow/leaderboard) for this benchmark and you can contribute to it by following the [Getting Started](https://mete.is/crow/getting-started) guide.

For more information on this benchmark, check the [website](https://mete.is/crow)

## Citation
```
@inproceedings{ismayilzada2023crow,
    title={CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks},
    author={Mete Ismayilzada and Debjit Paul and Syrielle Montariol and Mor Geva and Antoine Bosselut},
    booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    year={2023}
}
```