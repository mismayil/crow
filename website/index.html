---
layout: default
---

<div class="ui center aligned segment">
    <h3>
        <a href="https://mete.is" target="_blank">Mete Ismayilzada</a>, &nbsp;&nbsp; <a href="https://debjitpaul.github.io" target="_blank">Debjit Paul</a>, &nbsp;&nbsp; <a href="https://smontariol.github.io/" target="_blank">Syrielle Montariol</a>, <br/>&nbsp;&nbsp; <a href="https://mega002.github.io" target="_blank">Mor Geva</a><sup class="author-sup">*</sup>, &nbsp;&nbsp; <a href="https://atcbosselut.github.io" target="_blank">Antoine Bosselut</a>
    </h3>
    <p>EPFL, <sup class="author-sup">*</sup> Google DeepMind</p>
</div>

<div class="ui center aligned basic segment">
    <div class="ui large black label">
        <i class="paperclip icon"></i>
        <a href="http://arxiv.org/abs/2310.15239" target="_blank">Paper</a>
    </div>
    &nbsp;&nbsp;
    <div class="ui large teal label">
        <i class="github icon"></i>
        <a href="https://github.com/mismayil/crow" target="_blank">Code</a>
    </div>
</div>

<br><br>

<h2><span class="section-bar"></span>What is it?</h2>
<div class="ui middle aligned grid">
    <div class="sixteen wide column">
        <img src="images/crow-motivation.svg"/>
    </div>
    <div class="sixteen wide column">
        <div class="ui basic segment">
            <h3>CRoW is a multi-task benchmark to evaluate commonsense reasoning ability of AI models in solving real-world tasks where this ability is required.</h3>
            <p>The benchmark includes 6 diverse real-world NLP tasks:</p>
            <ul>
                <li>Open-domain Dialogue</li>
                <li>Dialogue Summarization</li>
                <li>Intent Detection</li>
                <li>Safety Detection</li>
                <li>Stance Classification</li>
                <li>Machine Translation (en-de, en-fr, en-ru, zh-en)</li>
            </ul>
        </div>
    </div>
</div>

<h2><span class="section-bar"></span>How is it built?</h2>
<div class="ui middle aligned grid">
    <div class="sixteen wide column">
        <img src="images/crow-pipeline.png"/>
    </div>
    <div class="sixteen wide column">
        <div class="ui basic segment">
            <p>
                We design a common multi-stage data collection pipeline for generating commonsense-based Winograd-style variations of examples, which can be applied to many tasks.
                This multi-stage approach has two key benefits. First, we can ground the perturbations to commonsense dimensions, ensuring the Winograd schemas differ on commonsense violations.
                Second, a particular stage can be skipped if the data for it is already available, which is the case for several tasks in our benchmark.
            </p>
        </div>
    </div>
</div>

<!-- <h2><span class="section-bar"></span>Why should you use it?</h2>
<p>If you are developing a system to perform one of the tasks included in our benchmark, </p> -->

<h2><span class="section-bar"></span>Cite</h2>
<div class="ui grid">
    <div class="column">
        <div class="ui basic segment">
            <div class="ui message">
                <pre>
                    <code style="color: grey">
@inproceedings{ismayilzada2023crow,
    title={CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks},
    author={Mete Ismayilzada and Debjit Paul and Syrielle Montariol and Mor Geva and Antoine Bosselut},
    booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    year={2023}
}
                    </code>
                </pre>
            </div>
            <p>Please also consider citing the original datasets used to construct this benchmark which can be found in <a href="/crow/tasks">Tasks</a> section.</p>
        </div>
    </div>
</div>